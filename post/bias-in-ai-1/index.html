<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>BIAS in AI (1) | BANKRUPT</title>
<link rel="shortcut icon" href="https://blog.xtopia.fun/favicon.ico?v=1622388527073">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://blog.xtopia.fun/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="BIAS in AI (1) | BANKRUPT - Atom Feed" href="https://blog.xtopia.fun/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700&display=swap" media="print" onload="this.media='all'">


<script>
  (function(a,b,c){var d=a.history,e=document,f=navigator||{},g=localStorage,
  h=encodeURIComponent,i=d.pushState,k=function(){return Math.random().toString(36)},
  l=function(){return g.cid||(g.cid=k()),g.cid},m=function(r){var s=[];for(var t in r)
  r.hasOwnProperty(t)&&void 0!==r[t]&&s.push(h(t)+"="+h(r[t]));return s.join("&")},
  n=function(r,s,t,u,v,w,x){var z="https://www.google-analytics.com/collect",
  A=m({v:"1",ds:"web",aip:c.anonymizeIp?1:void 0,tid:b,cid:l(),t:r||"pageview",
  sd:c.colorDepth&&screen.colorDepth?screen.colorDepth+"-bits":void 0,dr:e.referrer||
  void 0,dt:e.title,dl:e.location.origin+e.location.pathname+e.location.search,ul:c.language?
  (f.language||"").toLowerCase():void 0,de:c.characterSet?e.characterSet:void 0,
  sr:c.screenSize?(a.screen||{}).width+"x"+(a.screen||{}).height:void 0,vp:c.screenSize&&
  a.visualViewport?(a.visualViewport||{}).width+"x"+(a.visualViewport||{}).height:void 0,
  ec:s||void 0,ea:t||void 0,el:u||void 0,ev:v||void 0,exd:w||void 0,exf:"undefined"!=typeof x&&
  !1==!!x?0:void 0});if(f.sendBeacon)f.sendBeacon(z,A);else{var y=new XMLHttpRequest;
  y.open("POST",z,!0),y.send(A)}};d.pushState=function(r){return"function"==typeof d.onpushstate&&
  d.onpushstate({state:r}),setTimeout(n,c.delay||10),i.apply(d,arguments)},n(),
  a.ma={trackEvent:function o(r,s,t,u){return n("event",r,s,t,u)},
  trackException:function q(r,s){return n("exception",null,null,null,null,r,s)}}})
  (window,"UA-163788379-1",{anonymizeIp:true,colorDepth:true,characterSet:true,screenSize:true,language:true});
</script>



    <meta name="description" content="
When we say algorithm fairness, what we're talking about?

当我们在提及算法公平性的时候，我们在谈论什么？
来源：Safety, Bias, and Fairness (guest..." />
    <meta name="keywords" content="总结分享,人工智能,学术" />
    <link rel="stylesheet" href="https://s3.pstatp.com/cdn/expire-1-M/KaTeX/0.10.1/katex.min.css">
    <script src="https://s2.pstatp.com/cdn/expire-1-M/highlight.js/9.15.6/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/valine"></script>

<style>
	div#vcomments{
		width:100%;
		max-width: 800px;
		padding: 2.5%
	}
</style>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://blog.xtopia.fun">
  <img class="avatar" src="https://blog.xtopia.fun/images/avatar.png?v=1622388527073" alt="">
  </a>
  <h1 class="site-title">
    BANKRUPT
  </h1>
  <p class="site-description">
    在心裡與你握手😊
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          HOME
        </a>
      
    
      
        <a href="/archives" class="menu">
          ARCHIVES
        </a>
      
    
      
        <a href="/tags" class="menu">
          TAGS
        </a>
      
    
      
        <a href="/post/links" class="menu">
          LINKS
        </a>
      
    
      
        <a href="/post/images" class="menu">
          IMAGES
        </a>
      
    
      
        <a href="/post/about" class="menu">
          ABOUT
        </a>
      
    
  </div>
  <div style="text-align: center">
    <form id="gridea-search-form" style="position: relative" data-update="1622388527073" action="/search/index.html">
        <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
        <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
    </form>
</div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              BIAS in AI (1)
            </h2>
            <div class="post-info">
              <span>
                2020-12-20
              </span>
              <span>
                11 min read
              </span>
              
                <a href="https://blog.xtopia.fun/tag/dAaiEmiPH/" class="post-tag">
                  # 总结分享
                </a>
              
                <a href="https://blog.xtopia.fun/tag/jOyp8cX2S/" class="post-tag">
                  # 人工智能
                </a>
              
                <a href="https://blog.xtopia.fun/tag/n6T_3j_x3j/" class="post-tag">
                  # 学术
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/BIAS%20IN%20AI.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <blockquote>
<p>When we say algorithm fairness, what we're talking about?</p>
</blockquote>
<p>当我们在提及算法公平性的时候，我们在谈论什么？</p>
<p>来源：Safety, Bias, and Fairness (guest lecture by <em><a href="http://www.m-mitchell.com/">Margaret Mitchell</a></em>) [<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture19-bias.pdf">slides</a>] [<a href="https://www.youtube.com/watch?v=XR8YSRcuVLE&amp;feature=youtu.be">video</a>]<br>
Stanford / Winter 2019, <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/">CS224n: Natural Language Processing with Deep Learning</a></p>
<!-- more -->
<p>AI中的bias其实是相当多且广泛的，最普遍的是一个机器翻译的例子：</p>
<figure data-type="image" tabindex="1"><img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/bias%20in%20MT.jpg" alt="土耳其语和英语的互译" loading="lazy"></figure>
<p>这是一个土耳其语与英语互译的例子，机器会把原本是无性别的代词翻译为具有性别的代词。</p>
<p>最近在重新看CS224n，课程也正好邀请到了Google AI的研究学者Margaret Mitchell作关于Bias in AI的报告。这里也就将报告的大致内容梳理一下。</p>
<h1 id="bias-in-daily-life">Bias in Daily Life</h1>
<p>我们日常生活中就无时无刻充斥着偏见。接下来有几个例子说明。</p>
<h2 id="1-香蕉是什么">1. 香蕉是什么？ 🍌？</h2>
<p>这张图片上有什么？What do you see in this picture?</p>
<figure data-type="image" tabindex="2"><img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/Bananas.jpg" alt="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/Bananas.jpg" loading="lazy"></figure>
<p>你可能会说 ...</p>
<ul>
<li>香蕉</li>
<li>标签</li>
<li>摆在商店里的香蕉</li>
<li>放在货架上的香蕉</li>
<li>很多捆香蕉</li>
<li>贴上标签的香蕉</li>
<li>贴着标签的很多捆香蕉摆在商店里的货架上</li>
<li>...</li>
</ul>
<p>但很少会有人说：</p>
<p><code>这是“黄色的”香蕉</code></p>
<p>但当看见下面这两张图，我们会说这是“<strong>绿色的</strong>“香蕉、“<strong>成熟的</strong>”香蕉、“<strong>有斑点的</strong>”香蕉。</p>
<figure data-type="image" tabindex="3"><img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/bananas.jpg" alt="" loading="lazy"></figure>
<p>这是因为，在我们的意识里，“黄色”是香蕉是一种原型特征 (prototypical)，是一种基模和常识。</p>
<details>
  <summary>什么是Prototype?</summary>
  <br/>
<ul>
<li>Prototype Theory：One purpose of categorization is to <strong>reduce the infinite differences</strong> among stimuli <strong>to</strong> behaviourally and <strong>cognitively usable proportions</strong></li>
<li>There may be some central, prototypical notions of items that arise from stored typical properties for an object category (Rosch, 1975)</li>
<li>May also store exemplars (Wu &amp; Barsalou, 2009)</li>
</ul>
</details>
<br/>
<h2 id="2-a-simple-cognitive-test">2. A Simple Cognitive Test...</h2>
<p>下面是一个简单的认知测试：</p>
<p>小明的父亲开着一家杂货店。一天，小明从父亲的杂货店出来后被一辆迎面开来的车撞倒。他被送到了医院的急诊室。在手术台上，外科医生忽然发现，手术台上躺的竟是自己的儿子。请问小明与外科医生之间是什么关系？</p>
<details>
  <summary>这是几种可能的回答：</summary>
  <br/>
<ol>
<li>父子关系（he has two dads）</li>
<li>母子关系（he has a mother who's a doctor）</li>
</ol>
</details>
<br/>
<p>如果你不能迅速而准确地回答上面的问题，恐怕就是基模影响了认知判断过程。你需要仔细审视一下自己的思维过程：很可能问题出在你根据“常识”，做了一个虚假的前提假设一一外科医生都是男性。</p>
<figure data-type="image" tabindex="4"><img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/doctor%20&amp;%20famale%20doctor.jpg" alt="Doctor &amp; Female doctor" loading="lazy"></figure>
<p>事实上，大部分的测试者都忽略了医生是女性的可能性——包括男人、女人，甚至一些自认为是女性主义者的人。</p>
<blockquote>
<p>The majority of test subjects overlooked the possibility that the doctor is a she - including men, women, and self-described feminists.</p>
</blockquote>
<p>—— <a href="https://www.bu.edu/today/2014/bu-research-riddle-reveals-the-depth-of-gender-bias/">Wapman &amp; Belle, Boston University</a></p>
<h2 id="3-word-learning-from-text">3. Word Learning from Text</h2>
<p>我们谈论事物时就会不自觉地做出假设，它并不一定带有负面的意图，但这在一定程度上说明了事物的概念是如何在我们头脑中储存表示的，也反映了我们在世界上互动时是如何获得这些概念表示的。</p>
<p>这也会影响我们在文本中训练的结果。</p>
<p>这是一份2013年的工作[1]：</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>Frequency in corpus</th>
</tr>
</thead>
<tbody>
<tr>
<td>“spoke”</td>
<td>11,577,917</td>
</tr>
<tr>
<td>“laughed”</td>
<td>3,904,519</td>
</tr>
<tr>
<td>“murdered”</td>
<td>2,834,529</td>
</tr>
<tr>
<td>“inhaled”</td>
<td>984,613</td>
</tr>
<tr>
<td>“breathed”</td>
<td>725,034</td>
</tr>
<tr>
<td>“hugged”</td>
<td>610,040</td>
</tr>
<tr>
<td>“blinked”</td>
<td>390,692</td>
</tr>
<tr>
<td>“exhale”</td>
<td>168,985</td>
</tr>
</tbody>
</table>
<p>可以发现“murdered”(谋杀)的频率远大于“breathed”(呼吸)和“blinked”(眨眼睛)的频率。但在日常生活中，后两者的发生的频率会远高于前者。人们总是倾向于忽略一些生活中不言而喻的事情。</p>
<p>根据神经网络的特性，输出“murdered”的概率会远超于后两个词。</p>
<p>这种现象被称为<strong>Human Reporting Bias。</strong></p>
<blockquote>
<p>The frequency with which people write about actions, outcomes, or properties is not a reflection of real-world frequencies or the degree to which a property is characteristic of a class of individuals.</p>
</blockquote>
<p>人类关于行为、结果或属性的输出频率并不是真实世界频率的反映，也并不一定是一类事物的真实特征。</p>
<h2 id="这种偏见其实起源于我们的认知">这种偏见，其实起源于我们的认知。</h2>
<p><code>BIAS = BAD ??</code></p>
<p>这里必须要澄清的是，偏见并不一定是一件坏事情。</p>
<p>如果要考虑所有情况的排列组合，我们的大脑可能已经爆炸🤯🤯🤯了。有时候正因为我们拥有着一部分偏见，我们才可以应对这世上各式各样不同的事物，能够把它们变换为可以在现实生活中操作时可控制的东西。</p>
<blockquote>
<p>It's a trick our minds play on us in order to help us process the world.</p>
</blockquote>
<p>所以，偏见其实是有好有坏的。</p>
<p><code>“Bias” can be Good, Bad, Neutral</code></p>
<h1 id="bias-in-training-ai">Bias in Training AI</h1>
<h2 id="我们的训练过程就充斥着偏见bias">我们的训练过程，就充斥着偏见(Bias)</h2>
<p>首先让我们回忆一下AI的训练过程：</p>
<ol>
<li>收集和标注训练数据</li>
<li>训练模型</li>
<li>筛选、排序、聚合或生成结果</li>
<li>输出</li>
</ol>
<p>这其中的每一个步骤都充满着人类偏见：</p>
<figure data-type="image" tabindex="5"><img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/bias%20training.jpg" alt="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/bias%20training.jpg" loading="lazy"></figure>
<p>而偏见还会形成反馈循环。这被称为 <strong>Bias Network Effect</strong> 或者 <strong>Bias “Laundering”</strong>。</p>
<h2 id="bias-in-data">Bias in Data</h2>
<p>人类数据延续了人类的偏见。当ML从人类数据中学习时，结果是一个偏置网络效应。</p>
<p>常见的偏见有：</p>
<figure data-type="image" tabindex="6"><img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/human%20bias.jpg" alt="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/human%20bias.jpg" loading="lazy"></figure>
<p>more in : <a href="https://developers.google.com/machine-learning/glossary/">https://developers.google.com/machine-learning/glossary/</a></p>
<ul>
<li>Reporting bias (报告偏见)：人们报告的并不是真实世界频率的反映</li>
<li>Selection Bias (选择偏差)：人们的选择并不总是随机样本</li>
<li>Out-group homogeneity bias (外群体同质性偏见)：在比较态度、价值观、性格特征和其他特征时，人们倾向于认为外群体(outgroup)成员比内群体成员(ingroup)更相似。</li>
</ul>
<p>Bias in Data导致的结果有：</p>
<ul>
<li>具有偏差的数据表示 (Biased Data Representation)</li>
<li>具有偏差的标签 (Biased Labels)</li>
</ul>
<h2 id="bias-in-interpretation">Bias in Interpretation</h2>
<ul>
<li>Confirmation bias (确认偏见)：人们倾向于寻找、解释、支持和回忆信息，以确认先前存在的信念或假设</li>
<li>Overgeneralization (过度泛化)：根据过于笼统和/或不够具体的信息得出结论（也与过拟合有关）</li>
<li>Correlation fallacy (相关性谬误)：混淆相关性和因果关系</li>
<li>Automation bias (自动化偏差)：人们更倾向于来自自动化决策系统的建议</li>
</ul>
<h1 id="bias-in-ai">Bias in AI</h1>
<p>在AI中遇到的偏见大致可以分为这几类：</p>
<ul>
<li>在统计和机器学习中的偏差 (Bias in statistics and ML)
<ul>
<li>估计值的偏差：预测值与我们试图预测的正确值之间的差异</li>
<li>“偏差”一词 <em>b</em> (如y = mx + b)</li>
</ul>
</li>
<li>认知偏见 (Cognitive biases)
<ul>
<li>Confirmation bias, Recency bias, Optimism bias</li>
</ul>
</li>
<li>算法中的偏见 (Algorithmic bias)
<ul>
<li>对与种族、收入、性取向、宗教、性别和其他历史上与歧视和边缘化相关的特征相关的人的不公平、不公平或偏见待遇，会在算法系统或算法辅助决策中体现出来。</li>
</ul>
</li>
</ul>
<p>第一种的偏差是客观的可控制的，而第二种偏见来源于我们的现实生活，算法无法改变它。<br>
<strong>第三种偏见是需要我们注意和避免的</strong>。AI为人类服务，当AI的输入来源于人类提供的偏见数据（即使是无意的），计算机总是会<strong>放大这种偏见的想法</strong>。</p>
<blockquote>
<p>Although neural networks might be said to write their own programs, they do so towards goals set by humans, using data collected for human purposes. If the data is skewed, even by accident, the computers will <strong>amplify injustice</strong>.</p>
</blockquote>
<p><a href="https://www.theguardian.com/commentisfree/2016/oct/23/the-guardian-view-on-machine-learning-people-must-decide">—— The Guardian</a></p>
<h2 id="why-cannot-we-amplify-injustice">Why Cannot We Amplify Injustice?</h2>
<p>为什么说放大偏见是一件不好的事情？接下来有几个算法中放大偏见的例子。</p>
<p>第一个例子是<strong>预测性警务</strong> (Predicting Policing)。</p>
<p>算法会预测潜在的犯罪热点区域，可以帮助决策是否要在该区域增加军官部署。这种算法根据以前报告的<strong>逮捕地点</strong>（<strong>注意：并不是发生犯罪的地点</strong>）作为训练数据，从过去预测未来可能发生的犯罪地点。<br>
但有一些可能是犯罪的区域可能从来没有被探索到，这部分的可能性为0。而经过数据的循环反复训练，这使得情况会更加恶化。[2]</p>
<p>另一个例子是<strong>量刑辅助系统</strong> (Predicting Sentencing)。</p>
<p>经过训练，系统会默认认为黑人的犯罪风险高于白人。经过Automation Bias，过度泛化、循环反馈和相关性谬误，一些错误的例子反复发生，最后形成了某种因果关系。[3]</p>
<p>这样的例子数不胜数：</p>
<ul>
<li>
<p><a href="http://www.faception.com/">Faception</a>是是一家基于人的面部揭示其个性的公司。他们声称可以提供专业的引擎从脸的形象识别“高智商”、“白领犯罪”、“恋童癖”,和“恐怖分子”。其主要客户为国土安全和公共安全。</p>
<ul>
<li>但Google AI 的工作表明，即使是嘴角向上或向下扬起的角度也会影响其判断。[4] (See longer piece on Medium, “<a href="https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a">Physiognomy’s New Clothes</a>”)<br>
<img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/angle.jpg" alt="" loading="lazy"></li>
<li>这个例子中的偏见有：Selection Bias + Experimenter’s Bias + Confirmation Bias + Correlation Fallacy + Feedback Loops</li>
</ul>
</li>
<li>
<p>某工作声称他们发明了性取向探测器，他们通过从约会网站下爬取下来的35,326张照片预测同性恋者。[5]</p>
<blockquote>
<p>“Consistent with the prenatal hormone theory [PHT] of sexual orientation, gay men and women tended to have gender-atypical facial morphology.”<br>
<img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/homosexuality.jpg" alt="" loading="lazy"></p>
</blockquote>
<ul>
<li>但事实上，在自拍中，同性恋和异性恋之间的差异与打扮、表现和生活方式有关，也就是说，这是一种文化的差异，而不是面部结构的差异。甚至可以通过构造一颗简单的打扮特征的决策树来判断。(See longer response on Medium, <a href="https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477">“Do Algorithms Reveal Sexual Orientation or Just Expose our Stereotypes?”</a>)<br>
<img src="https://xtopia-1258297046.cos.ap-shanghai.myqcloud.com/gromming.jpg" alt="" loading="lazy"></li>
<li>这个例子中的偏见有：Selection Bias + Experimenter’s Bias + Correlation Fallacy</li>
</ul>
</li>
<li>
<p>...</p>
</li>
</ul>
<p>总而言之，算法偏见十分常见。但如果AI做不到无偏见，我们又怎么能够相信他们的判断？<br>
那么我们该怎样做，才能避免算法偏见？当我们在提及算法公平性的时候，我们在谈论什么？</p>
<hr>
<h1 id="reference">Reference</h1>
<p>[1] Jonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction (AKBC '13). DOI:<a href="10.1145/2509558.2509563">https://doi.org/10.1145/2509558.2509563</a></p>
<p>[2] <a href="https://www.smithsonianmag.com/innovation/artificial-intelligence-is-now-used-predict-crime-is-it-biased-180968337/">Smithsonian. Artificial Intelligence Is Now Used to Predict Crime. But Is It Biased? 2018</a></p>
<p>[3] <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">ProPublica. Northpointe: Risk in Criminal Sentencing. 2016.</a></p>
<p>[4] Xiaolin Wu, Xi Zhang: Automated Inference on Criminality using Face Images. <a href="https://arxiv.org/abs/1611.04135">CoRR abs/1611.04135 (2016)</a></p>
<p>[5] Wang, Yilun, and Michal Kosinski. 2017. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” PsyArXiv. September 7. doi:<a href="https://psyarxiv.com/hv28a/">10.1037/pspa0000098</a>.</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#bias-in-daily-life">Bias in Daily Life</a>
<ul>
<li><a href="#1-%E9%A6%99%E8%95%89%E6%98%AF%E4%BB%80%E4%B9%88">1. 香蕉是什么？ 🍌？</a></li>
<li><a href="#2-a-simple-cognitive-test">2. A Simple Cognitive Test...</a></li>
<li><a href="#3-word-learning-from-text">3. Word Learning from Text</a></li>
<li><a href="#%E8%BF%99%E7%A7%8D%E5%81%8F%E8%A7%81%E5%85%B6%E5%AE%9E%E8%B5%B7%E6%BA%90%E4%BA%8E%E6%88%91%E4%BB%AC%E7%9A%84%E8%AE%A4%E7%9F%A5">这种偏见，其实起源于我们的认知。</a></li>
</ul>
</li>
<li><a href="#bias-in-training-ai">Bias in Training AI</a>
<ul>
<li><a href="#%E6%88%91%E4%BB%AC%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%B0%B1%E5%85%85%E6%96%A5%E7%9D%80%E5%81%8F%E8%A7%81bias">我们的训练过程，就充斥着偏见(Bias)</a></li>
<li><a href="#bias-in-data">Bias in Data</a></li>
<li><a href="#bias-in-interpretation">Bias in Interpretation</a></li>
</ul>
</li>
<li><a href="#bias-in-ai">Bias in AI</a>
<ul>
<li><a href="#why-cannot-we-amplify-injustice">Why Cannot We Amplify Injustice?</a></li>
</ul>
</li>
<li><a href="#reference">Reference</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://blog.xtopia.fun/post/unfinished-and-untested-lost-love/">
              <h3 class="post-title">
                Unfinished &amp; Untested, Lost Love
              </h3>
            </a>
          </div>
        

        


      <div id="vcomments"></div>
      <script>
          new Valine({
              el: '#vcomments',
              lang: 'zh-cn',//设置评论语言
              app_id: '0glHbQOVYrwnrCDsjkeXG0CQ-gzGzoHsz',			    										   	
              app_key: 'ev2DY4ktRLwNGlTD3VT3bkSS',    										      
              placeholder: 'Write a comment',  // 提示符
              max_nest: 2,					// 嵌套评论深度，太大会影响加载速度
              page_size: 5					// 分页大小
          })
      </script>

        <div class="site-footer">
  此处是通往何方的小道. 
  <a class="rss" href="https://blog.xtopia.fun/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>


      </div>

    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
